# -*- coding: utf-8 -*-
import os
import json
import time
import re
import random
import argparse
import requests
from urllib.parse import urlparse
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import gspread

# =========================
# æ—¢å®šï¼ˆå¼•æ•°/ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰
# =========================
DEFAULT_KEYWORD = "ãƒ›ãƒ³ãƒ€"
DEFAULT_SPREADSHEET_ID = "1AwwMGKMHfduwPkrtsik40lkO1z1T8IU_yd41ku-yPi8"

# =========================
# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def format_datetime(dt_obj: datetime) -> str:
    return dt_obj.strftime("%Y/%m/%d %H:%M")

TIME_RE = re.compile(r"(\d+)\s*(åˆ†|æ™‚é–“|æ—¥)\s*å‰")
TIME_ONLY_RE = re.compile(r"^\s*(\d+)\s*(åˆ†|æ™‚é–“|æ—¥)\s*(å‰|)?\s*$")

def parse_relative_time(pub_label: str, base_time: datetime) -> str:
    if not pub_label:
        return "å–å¾—ä¸å¯"
    pub_label = pub_label.strip().lower()
    try:
        if "åˆ†å‰" in pub_label or "minute" in pub_label:
            m = re.search(r"(\d+)", pub_label)
            if m:
                dt = base_time - timedelta(minutes=int(m.group(1)))
                return format_datetime(dt)
        elif "æ™‚é–“å‰" in pub_label or "hour" in pub_label:
            h = re.search(r"(\d+)", pub_label)
            if h:
                dt = base_time - timedelta(hours=int(h.group(1)))
                return format_datetime(dt)
        elif "æ—¥å‰" in pub_label or "day" in pub_label:
            d = re.search(r"(\d+)", pub_label)
            if d:
                dt = base_time - timedelta(days=int(d.group(1)))
                return format_datetime(dt)
        elif re.match(r'\d+æœˆ\d+æ—¥', pub_label):
            dt = datetime.strptime(pub_label, "%mæœˆ%dæ—¥").replace(year=base_time.year)
            return format_datetime(dt)
        elif re.match(r'\d{4}/\d{1,2}/\d{1,2}', pub_label):
            dt = datetime.strptime(pub_label, "%Y/%m/%d")
            return format_datetime(dt)
        elif re.match(r'\d{1,2}:\d{2}', pub_label):
            t = datetime.strptime(pub_label, "%H:%M").time()
            dt = datetime.combine(base_time.date(), t)
            if dt > base_time:
                dt -= timedelta(days=1)
            return format_datetime(dt)
    except:
        pass
    return "å–å¾—ä¸å¯"

def get_last_modified_datetime(url: str) -> str:
    try:
        res = requests.head(url, timeout=5, allow_redirects=True)
        if 'Last-Modified' in res.headers:
            dt = parsedate_to_datetime(res.headers['Last-Modified'])
            jst = dt + timedelta(hours=9)
            return format_datetime(jst)
    except:
        pass
    return "å–å¾—ä¸å¯"

def make_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1280,2000")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    )
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def resolve_final_url(url: str) -> str:
    """Google News ç­‰ã®ä¸­é–“URLã‚’æœ€çµ‚URLã«è§£æ±ºï¼ˆæˆåŠŸæ™‚ã®ã¿ç½®æ›ï¼‰"""
    try:
        parsed = urlparse(url)
        if "news.google.com" in parsed.netloc:
            res = requests.get(url, timeout=6, allow_redirects=True)
            if res.ok:
                return res.url
    except:
        pass
    return url

def publisher_from_url(url: str) -> str:
    """URLã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰åª’ä½“åã‚’æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
    try:
        netloc = urlparse(url).netloc.lower()
        if not netloc:
            return ""
        if netloc.endswith("msn.com"):
            return "MSN"
        if "news.yahoo.co.jp" in netloc:
            return "Yahoo"
        host = netloc.split(":")[0]
        if host.startswith("www."):
            host = host[4:]
        NAME_MAP = {
            "response.jp": "ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆResponse.jpï¼‰",
            "newsweekjapan.jp": "ãƒ‹ãƒ¥ãƒ¼ã‚ºã‚¦ã‚£ãƒ¼ã‚¯æ—¥æœ¬ç‰ˆ",
            "bloomberg.co.jp": "ãƒ–ãƒ«ãƒ¼ãƒ ãƒãƒ¼ã‚°",
            "motor-fan.jp": "Motor-Fan",
            "young-machine.com": "ãƒ¤ãƒ³ã‚°ãƒã‚·ãƒ³",
            "as-web.jp": "autosport web",
            "webcg.net": "WebCG",
            "bestcarweb.jp": "ãƒ™ã‚¹ãƒˆã‚«ãƒ¼Web",
        }
        if host in NAME_MAP:
            return NAME_MAP[host]
        parts = host.split(".")
        base = parts[-2] if len(parts) >= 2 else host
        base = base.replace("-", " ").replace("_", " ")
        return base.capitalize()
    except:
        return ""

def clean_source_text(raw: str) -> str:
    """
    'Merkmalï¼ˆãƒ¡ãƒ«ã‚¯ãƒãƒ¼ãƒ«ï¼‰ 1 æ™‚é–“' â†’ 'Merkmalï¼ˆãƒ¡ãƒ«ã‚¯ãƒãƒ¼ãƒ«ï¼‰'
    'MSN ã«ã‚ˆã‚‹é…ä¿¡ 1 åˆ†' â†’ ''ï¼ˆ= å¾Œæ®µã§URLã‹ã‚‰æ¨å®šï¼‰
    å˜ç‹¬ã® '4 æ—¥' '7 æ™‚é–“' â†’ ''ï¼ˆ= å¾Œæ®µã§URLã‹ã‚‰æ¨å®šï¼‰
    """
    if not raw:
        return ""
    t = raw.strip()
    t = re.sub(r"\bon\s+MSN\b", "", t, flags=re.IGNORECASE)     # "on MSN"
    t = re.sub(r"MSN\s*ã«ã‚ˆã‚‹é…ä¿¡", "", t)                        # "MSN ã«ã‚ˆã‚‹é…ä¿¡"
    t = re.sub(r"(æä¾›|é…ä¿¡)\s*[:ï¼š]?", "", t)                   # "æä¾›:","é…ä¿¡:"
    t = t.replace("ãƒ»", " ").replace("â€¢", " ").replace("Â·", " ")
    t = re.sub(r"\s*\d+\s*(åˆ†|æ™‚é–“|æ—¥)\s*(å‰|)?\s*$", "", t).strip()  # æœ«å°¾ã®æ™‚é–“è¡¨ç¾
    t = TIME_RE.sub("", t).strip()                                    # æ®‹å­˜ã®æ™‚é–“è¡¨ç¾
    t = re.sub(r"\s*\(\s*\)\s*$", "", t).strip()                      # ç©ºæ‹¬å¼§
    t = re.sub(r"\s{2,}", " ", t).strip()
    t = t.strip("ãƒ»|â€¢|Â·|-â€“â€”:ï¼š").strip()
    if TIME_ONLY_RE.match(t):
        return ""
    return t

def is_timeish(text: str) -> bool:
    """æ™‚åˆ»ã£ã½ã„ã ã‘ã®æ–‡å­—åˆ—ã‹åˆ¤å®š"""
    if not text:
        return False
    if TIME_ONLY_RE.match(text.strip()):
        return True
    if TIME_RE.search(text):
        return True
    return False

# =========================
# å„ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘
# =========================
def get_google_news_with_selenium(keyword: str) -> list[dict]:
    driver = make_driver()
    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    driver.get(url)
    time.sleep(5)
    for _ in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.2)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data: list[dict] = []
    for article in soup.find_all("article"):
        try:
            a_tag = article.select_one("a.JtKRv")
            time_tag = article.select_one("time.hvbAAd")
            if not a_tag or not time_tag:
                continue
            title = a_tag.get_text(strip=True)
            href = a_tag.get("href") or ""
            url = "https://news.google.com" + href[1:] if href.startswith("./") else href
            final_url = resolve_final_url(url)
            guessed_source = publisher_from_url(final_url)

            source = ""
            for sel in ["div.vr1PYe", "div.UOVeFe", "a.wEwyrc"]:
                el = article.select_one(sel)
                if el:
                    source = el.get_text(strip=True)
                    break
            if not source:
                source = guessed_source or "Google"

            dt = datetime.strptime(time_tag.get("datetime"), "%Y-%m-%dT%H:%M:%SZ") + timedelta(hours=9)
            pub_date = format_datetime(dt)

            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": final_url, "æŠ•ç¨¿æ—¥": pub_date, "å¼•ç”¨å…ƒ": source})
        except:
            continue
    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    driver = make_driver()
    search_url = (
        f"https://news.yahoo.co.jp/search?p={keyword}"
        f"&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(search_url)
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data: list[dict] = []
    items = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    for li in items:
        try:
            title_tag = li.find("div", class_=re.compile("sc-3ls169-0"))
            link_tag = li.find("a", href=True)
            time_tag = li.find("time")

            title = title_tag.get_text(strip=True) if title_tag else ""
            url = link_tag["href"] if link_tag else ""
            date_str = time_tag.get_text(strip=True) if time_tag else ""

            pub_date = "å–å¾—ä¸å¯"
            if date_str:
                ds = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip()
                try:
                    pub_date = format_datetime(datetime.strptime(ds, "%Y/%m/%d %H:%M"))
                except:
                    pub_date = ds

            source = ""
            for sel in [
                "div.sc-n3vj8g-0.yoLqH div.sc-110wjhy-8.bsEjY span",
                "div.sc-n3vj8g-0.yoLqH",
                "span",
                "div"
            ]:
                el = li.select_one(sel)
                if el:
                    txt = el.get_text(" ", strip=True)
                    txt = re.sub(r"\d{4}/\d{1,2}/\d{1,2} \d{2}:\d{2}", "", txt)
                    txt = re.sub(r"\([^)]+\)", "", txt)
                    txt = txt.strip()
                    if txt and not txt.isdigit() and any(ch.isalpha() or '\u3040' <= ch <= '\u9FFF' for ch in txt):
                        source = txt
                        break
            if not source:
                source = publisher_from_url(url) or "Yahoo"

            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub_date, "å¼•ç”¨å…ƒ": source})
        except:
            continue
    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

def get_msn_news_with_selenium(keyword: str) -> list[dict]:
    """
    MSN(Bingãƒ‹ãƒ¥ãƒ¼ã‚¹) å¼·åŒ–ç‰ˆï¼š
    - CookieåŒæ„å¯¾å¿œ
    - a.title / a[data-title] ä¸¡å¯¾å¿œ
    - å‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åª’ä½“åã¨ç›¸å¯¾æ™‚åˆ»ã‚’åˆ†é›¢æŠ½å‡º
    - 'on MSN' ã‚„ 'â—¯æ™‚é–“å‰' ã‚’å¼•ç”¨å…ƒã‹ã‚‰é™¤å»
    - å–ã‚Œãªã„æ—¥æ™‚ã¯ Last-Modified ã§è£œå®Œ
    """
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException

    now = datetime.utcnow() + timedelta(hours=9)
    driver = make_driver()
    url = (
        f"https://www.bing.com/news/search?q={keyword}"
        "&qft=sortbydate%3D%271%27"
        "&setlang=ja&cc=JP&FORM=HDRSC6"
    )
    driver.get(url)

    # CookieåŒæ„
    try:
        WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.ID, "bnp_btn_accept"))
        ).click()
    except TimeoutException:
        pass

    # è¨˜äº‹èª­ã¿è¾¼ã¿å¾…ã¡
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "a.title, a[data-title]"))
        )
    except TimeoutException:
        time.sleep(2)

    # Lazy Loadå¯¾ç­–ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
    for _ in range(4):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.0)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data: list[dict] = []
    anchors = soup.select("a.title, a[data-title]")
    for a in anchors:
        try:
            title = (a.get("data-title") or a.get_text(strip=True) or "").strip()
            href = a.get("href") or ""
            if not (title and href):
                continue

            parent = a.find_parent(["div", "li"]) or a.parent

            # 1) sourceãƒ–ãƒ­ãƒƒã‚¯ã®æ–‡å­—åˆ—ã‚’å–å¾—
            raw_source = ""
            if parent:
                s_el = parent.select_one("div.source, span.source")
                if s_el:
                    raw_source = s_el.get_text(" ", strip=True)

            # 2) ç›¸å¯¾æ™‚åˆ» or ISO datetime ã‚’æ‹¾ã†ï¼ˆæŠ•ç¨¿æ—¥ç”¨ï¼‰
            label = ""
            if parent:
                for el in parent.select("[aria-label]"):
                    lab = el.get("aria-label", "").strip()
                    if TIME_RE.search(lab):
                        label = lab
                        break
            if not label and parent:
                for el in parent.select("time"):
                    t = (el.get_text(strip=True) or "").strip()
                    if TIME_RE.search(t):
                        label = t
                        break
                    if el.get("datetime"):
                        label = el.get("datetime")
                        break

            pub_date = "å–å¾—ä¸å¯"
            if label:
                if "T" in label and ":" in label and label.endswith("Z"):
                    try:
                        dt = datetime.strptime(label, "%Y-%m-%dT%H:%M:%SZ") + timedelta(hours=9)
                        pub_date = format_datetime(dt)
                    except:
                        pass
                else:
                    pub_date = parse_relative_time(label, now)
            if pub_date == "å–å¾—ä¸å¯":
                pub_date = get_last_modified_datetime(href)

            # 3) å¼•ç”¨å…ƒã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° â†’ å‘¨è¾ºå€™è£œ â†’ URLæ¨å®š ã®ä¸‰æ®µæ§‹ãˆ
            source = clean_source_text(raw_source)
            if (not source) and parent:
                for sel in ["cite", "span.provider", "div.provider", "span.source", "div.source a"]:
                    el = parent.select_one(sel)
                    if el:
                        cand = clean_source_text(el.get_text(" ", strip=True))
                        if cand and not is_timeish(cand):
                            source = cand
                            break
            if (not source) or is_timeish(source):
                source = publisher_from_url(href) or "MSN"

            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": href, "æŠ•ç¨¿æ—¥": pub_date, "å¼•ç”¨å…ƒ": source})
        except:
            continue

    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

# =========================
# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿
# =========================
def write_to_spreadsheet(articles: list[dict], spreadsheet_id: str, worksheet_name: str):
    """
    æ—¢å­˜URLã¨é‡è¤‡ã—ãªã„ã‚‚ã®ã ã‘è¿½è¨˜ã€‚ã‚·ãƒ¼ãƒˆãŒç„¡ã‘ã‚Œã°ä½œæˆã€‚
    èªè¨¼: GCP_SERVICE_ACCOUNT_KEYï¼ˆç’°å¢ƒï¼‰ or credentials.jsonï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
    """
    credentials_json_str = os.environ.get('GCP_SERVICE_ACCOUNT_KEY')
    credentials = json.loads(credentials_json_str) if credentials_json_str else json.load(open('credentials.json'))
    gc = gspread.service_account_from_dict(credentials)

    for attempt in range(5):
        try:
            sh = gc.open_by_key(spreadsheet_id)
            try:
                ws = sh.worksheet(worksheet_name)
            except gspread.exceptions.WorksheetNotFound:
                ws = sh.add_worksheet(title=worksheet_name, rows="1", cols="4")
                ws.append_row(['ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'æŠ•ç¨¿æ—¥', 'å¼•ç”¨å…ƒ'])

            existing = ws.get_all_values()
            existing_urls = set(row[1] for row in existing[1:] if len(row) > 1)

            new_rows = [[a['ã‚¿ã‚¤ãƒˆãƒ«'], a['URL'], a['æŠ•ç¨¿æ—¥'], a['å¼•ç”¨å…ƒ']]
                        for a in articles if a['URL'] not in existing_urls]

            if new_rows:
                ws.append_rows(new_rows, value_input_option='USER_ENTERED')
                print(f"âœ… {len(new_rows)}ä»¶ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã€Œ{worksheet_name}ã€ã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
            else:
                print(f"âš ï¸ è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ï¼ˆ{worksheet_name}ï¼‰")
            return
        except gspread.exceptions.APIError as e:
            print(f"âš ï¸ Google API Error (attempt {attempt + 1}/5): {e}")
            time.sleep(5 + random.random() * 5)

    raise RuntimeError("âŒ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ5å›è©¦è¡Œã—ã¦ã‚‚æˆåŠŸã›ãšï¼‰")

# =========================
# è¨­å®šã®è§£æ±ºï¼ˆå¼•æ•°/ç’°å¢ƒ/æ—¢å®šï¼‰
# =========================
def resolve_config() -> tuple[str, str]:
    parser = argparse.ArgumentParser()
    parser.add_argument("--keyword", type=str, default=None, help="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: ãƒ›ãƒ³ãƒ€ï¼‰")
    parser.add_argument("--sheet", type=str, default=None, help="ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID")
    args = parser.parse_args()

    keyword = args.keyword or os.getenv("NEWS_KEYWORD") or DEFAULT_KEYWORD
    spreadsheet_id = args.sheet or os.getenv("SPREADSHEET_ID") or DEFAULT_SPREADSHEET_ID
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
    print(f"ğŸ“„ SPREADSHEET_ID: {spreadsheet_id}")
    return keyword, spreadsheet_id

# =========================
# ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# =========================
if __name__ == "__main__":
    keyword, spreadsheet_id = resolve_config()

    print("\n--- Google News ---")
    google_news_articles = get_google_news_with_selenium(keyword)
    if google_news_articles:
        write_to_spreadsheet(google_news_articles, spreadsheet_id, "Google")

    print("\n--- Yahoo! News ---")
    yahoo_news_articles = get_yahoo_news_with_selenium(keyword)
    if yahoo_news_articles:
        write_to_spreadsheet(yahoo_news_articles, spreadsheet_id, "Yahoo")

    print("\n--- MSN News ---")
    msn_news_articles = get_msn_news_with_selenium(keyword)
    if msn_news_articles:
        write_to_spreadsheet(msn_news_articles, spreadsheet_id, "MSN")
